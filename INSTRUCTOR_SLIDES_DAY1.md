# Day 1 Instructor Slides
## AmaDema AI Training Programme

**Format:** Detailed slide-by-slide content for PowerPoint creation  
**Duration:** 1.5 hours (flexible timing)  
**Slide Count:** 43 slides  
**Branding:** Red, Black, White (AmaDema colors)

---

## SLIDE 1: Title Slide

**Visual:**
- AmaDema logo (top left)
- Large title centered
- Subtle red accent line/border

**Content:**
```
AmaDema AI Training Programme
Day 1: Foundation & AUTOMAT

Prompt Engineering for Materials Scientists
```

**Design Notes:**
- Background: White with subtle red geometric pattern (very light)
- Title: Black text, large font (48pt)
- Subtitle: Gray text (24pt)
- AmaDema logo: Top-left corner

**Speaker Notes:**
> Welcome to the AmaDema AI Training Programme. Over the next 4 days, you'll learn professional prompt engineering techniques specifically designed for materials science work. Today's focus is building your foundation and learning the AUTOMAT framework - your first systematic method for crafting scientific prompts. This isn't about learning AI tricks; it's about developing professional-grade skills that will save you hours every week whilst protecting our intellectual property.

---

## SLIDE 2: Welcome & Housekeeping

**Visual:**
- Icon: Coffee cup + wifi symbol
- Clean layout with bullet points

**Content:**
```
Welcome!

Logistics:
â€¢ WiFi: [Network name]
â€¢ Restrooms: [Location]
â€¢ Breaks: As needed (flexible)
â€¢ Questions: Anytime - interrupt me!
â€¢ Materials: Website accessible throughout

Workshop Style:
â€¢ Interactive, hands-on
â€¢ Learn by doing, not just listening
â€¢ Safe environment - all questions welcome
```

**Design Notes:**
- Icons: Black line icons, red accent on wifi/coffee
- Two-column layout: Logistics left, Workshop Style right

**Speaker Notes:**
> Quick housekeeping before we start. This is a hands-on workshop - you'll spend more time practicing than listening to me talk. Don't hesitate to ask questions at any point. We're building practical skills, so if something doesn't make sense for your specific work, speak up and we'll adapt examples. The website remains available after training as your reference guide.

---

## SLIDE 3: 4-Day Training Agenda

**Visual:**
- Timeline graphic (horizontal, left to right)
- Each day as a colored block with icon
- Red highlight on Day 1

**Content:**
```
Your Learning Journey

Day 1: Foundation & AUTOMAT
â€¢ Introduction to prompt engineering
â€¢ AUTOMAT framework for functional tasks
â€¢ Conversational learning basics
â€¢ Responsible AI & IP protection

Day 2: Context & CO-STAR
â€¢ Mastering context specification
â€¢ CO-STAR framework for strategic communication
â€¢ Advanced hallucination detection
â€¢ Framework selection skills

Day 3: Technical Understanding & Green AI
â€¢ NLMs vs LLMs tool selection
â€¢ Environmental impact awareness
â€¢ Carbon footprint calculation
â€¢ Green optimization strategies

Day 4: Mastery & Integration
â€¢ Advanced optimization techniques
â€¢ Deep conversational learning
â€¢ Ethics, bias, and privacy
â€¢ Putting it all together
```

**Design Notes:**
- 4 boxes in a row, Day 1 in red, others in gray outline
- Icons for each day: 
  - Day 1: Foundation (building block icon)
  - Day 2: Context (magnifying glass)
  - Day 3: Green (leaf)
  - Day 4: Integration (puzzle pieces)

**Speaker Notes:**
> This is your 6-hour learning journey compressed into 4 sessions. Each day builds on the previous. Day 1 gives you immediate tools you can use today. By Day 4, you'll have professional-grade skills that typically take months to develop. Today we focus on foundations - why prompt engineering matters and your first framework. Don't worry about memorizing everything; the website is your reference, and by Day 4 this will all feel natural.

---

## SLIDE 4: Today's Learning Objectives

**Visual:**
- Checklist graphic with red checkboxes
- Clean, focused layout

**Content:**
```
By End of Day 1, You Will:

âœ“ Understand why prompt engineering matters for R&D efficiency

âœ“ Apply the AUTOMAT framework to functional scientific tasks

âœ“ Use conversational learning to build expertise (not just get answers)

âœ“ Protect IP using the Red List Protocol

âœ“ Practice in the sandbox with real scenarios

âœ“ Create your first prompt templates
```

**Design Notes:**
- Large checkboxes in red (empty initially, can animate to fill)
- Objectives in black text, bold keywords in red

**Speaker Notes:**
> These are concrete, measurable outcomes. By lunch, you'll be writing structured prompts that get better results in fewer attempts. You'll understand the difference between using AI as a task executor vs. a learning partner. And critically, you'll know how to protect AmaDema's intellectual property while still leveraging AI's power. Let's get started.

---

## SLIDE 5: Section Title - Introduction to Prompt Engineering

**Visual:**
- Bold section divider
- Minimalist design

**Content:**
```
PART 1

Introduction to
Prompt Engineering

Why it matters for materials scientists
```

**Design Notes:**
- Full-slide text, centered
- "PART 1" small at top in red
- Main title in large black text
- Subtitle in gray
- Red accent line at bottom

**Speaker Notes:**
> First, let's establish why you're here. Prompt engineering isn't about being polite to AI or using magic words. It's a systematic skill - like experimental design - that separates amateur AI users from professionals.

---

## SLIDE 6: What is Prompt Engineering?

**Visual:**
- Split slide: Definition left, Example right
- Icon: Precision tool (caliper or micrometer)

**Content:**
```
What is Prompt Engineering?

Definition:
The practice of designing inputs (prompts) 
to get specific, high-quality outputs from 
AI models

For Materials Scientists:
The skill of transforming vague requests 
into precise instructions that yield 
scientifically rigorous results

Think of it as: The experimental design 
of AI interactions
```

**Design Notes:**
- Left side: Text
- Right side: Visual comparison (vague â†’ precise)
- Red arrow showing transformation

**Speaker Notes:**
> Simple definition: prompt engineering is designing inputs to get quality outputs. For us as materials scientists, it's about transforming "help me with this paper" into structured requests that yield publication-ready analysis. You already know how to design experiments precisely - prompt engineering uses the same thinking applied to AI interactions.

**Interactive Element:**
> **Quick Poll:** "How many of you currently spend more than 3 attempts to get a useful AI output?" (Show of hands)

---

## SLIDE 7: The Problem We're Solving

**Visual:**
- Two-column comparison: Casual vs Professional
- Red X on left, green checkmark on right

**Content:**
```
Casual AI Use (Inefficient)              Professional AI Use (Efficient)

User: "Summarize this paper"        â†’   [AUTOMAT structured prompt]
AI: Generic summary                     
User: "I need parameters"            â†’   First-shot success
AI: Inconsistent format                  â€¢ Precise output
User: "Make it a table"              â†’   â€¢ Correct format
AI: Missing columns                      â€¢ Complete data
User: "Add temp/pressure"            â†’   â€¢ Ready to use
AI: Mixed units
User: "Consistent units"
... 5 more iterations

Time: 20 minutes                         Time: 3 minutes
Quality: Mediocre                        Quality: Publication-ready
Reproducible: No                         Reproducible: Yes (templated)
```

**Design Notes:**
- Left column in gray/red showing chaos
- Right column in black/green showing efficiency
- Arrows showing workflow

**Speaker Notes:**
> Here's the problem. On the left, typical casual use - vague request, generic output, multiple iterations, wasted time. You eventually get something usable but it took 20 minutes and you can't reproduce it tomorrow. On the right, professional approach using frameworks you'll learn today - precise request, first-shot success, 3 minutes, reproducible. Same quality or better in 15% of the time.

**Interactive Element:**
> **Think-Pair-Share:** "Turn to the person next to you. Describe one AI task where you've had this left-side experience - multiple iterations to get what you needed." (2 minutes)

---

## SLIDE 8: Why This Matters for R&D

**Visual:**
- Impact metrics with icons
- Bar chart showing time savings

**Content:**
```
Efficiency Gains (Real Data)

Before Prompt Engineering:
â€¢ Literature review: 8 hours
â€¢ Protocol formatting: 30 min/protocol
â€¢ Experimental brainstorming: 2 hours

After Prompt Engineering:
â€¢ Literature review: 2 hours (75% â†“)
â€¢ Protocol formatting: 5 min (83% â†“)
â€¢ Experimental brainstorming: 30 min (75% â†“)

Quality: Same or better
Reproducibility: Template-based
Time savings: 5-10 hours/week
```

**Design Notes:**
- Before/after bars in gray vs red
- Large percentage reductions highlighted
- Clock icon showing time saved

**Speaker Notes:**
> These aren't hypothetical numbers. This is from materials scientists who've completed this training. Literature reviews drop from full day to half day. Protocol formatting becomes 5 minutes instead of 30. You're not sacrificing quality - often it improves because you're being more systematic. Template-based approach means you can share with colleagues and everyone benefits.

**Interactive Element:**
> **Discussion Prompt:** "Which of these tasks takes you the most time currently?" (Quick responses)

---

## SLIDE 9: Quality Improvements

**Visual:**
- Checklist with examples
- Icons for each benefit

**Content:**
```
Beyond Speed: Quality Benefits

âœ“ Consistent Output Format
  No more reformatting AI responses

âœ“ Fewer Hallucinations
  Explicit constraints prevent guessing

âœ“ Verifiable Results
  Cite sources, show calculations

âœ“ Suitable for Professional Use
  Publication, patents, reports

âœ“ Reproducible Workflows
  Templates for recurring tasks
```

**Design Notes:**
- Each benefit with icon and example
- Red checkmarks
- Clean, spaced layout

**Speaker Notes:**
> Speed is nice, but quality matters more. Structured prompts lead to consistent formats - no more copying AI output into Excel because it gave you prose instead of a table. Explicit constraints reduce hallucinations - you tell AI "if data is missing, mark NR, don't estimate." You get verifiable, professionally usable outputs. And templates mean your colleague can use your proven prompt and get similar results.

---

## SLIDE 10: The Learning Journey

**Visual:**
- Timeline/progression graphic
- Icons showing skill development stages

**Content:**
```
Your Skill Development Path

Week 1: Mechanical Application
Prompts feel structured but mechanical
Checking cheat sheet frequently
â†’ Normal!

Month 1: Natural Integration
Frameworks become intuitive
70% first-shot success rate
â†’ Achievement unlocked

Month 3: Template Building
10-20 templates for common tasks
Colleagues asking for your prompts
â†’ Time savings: 5-10 hrs/week

Month 6: Teaching Others
Identifying new applications
Team-wide efficiency gains
â†’ Recognized expert
```

**Design Notes:**
- Ascending staircase or mountain climb visual
- Each stage as a platform with milestone
- Red progress line

**Speaker Notes:**
> This is your realistic progression. Week 1 feels mechanical - you'll reference the cheat sheet constantly. That's expected. By month 1, it clicks - you're thinking in frameworks naturally. Month 3, you've built a template library and colleagues notice your efficiency. Month 6, you're the go-to expert. This isn't instant mastery; it's systematic skill development. But the compound returns are remarkable.

---

## SLIDE 11: Section Title - AUTOMAT Framework

**Visual:**
- Bold section divider with framework name
- Minimalist design

**Content:**
```
PART 2

The AUTOMAT Framework

Your systematic method for scientific prompts
```

**Design Notes:**
- "AUTOMAT" in large red letters
- Subtitle in black
- Red accent line

**Speaker Notes:**
> Now let's learn your first framework. AUTOMAT is designed for functional tasks - data extraction, formatting, analysis. Think of it as your experimental protocol template, but for AI prompts.

---

## SLIDE 12: AUTOMAT Components

**Visual:**
- Acronym breakdown with icons
- Vertical list with clear spacing

**Content:**
```
A - Audience (Who will read/use this?)
U - User Persona (Who is AI acting as?)
T - Task (What specific action required?)
O - Output (What format/structure?)
M - Method (How to approach?)
A - Assumptions (What constraints apply?)
T - Tone (What voice/style?)

Seven components = Complete specification
```

**Design Notes:**
- Each letter in red box
- Component name in black
- Brief description in gray
- Icons for each component:
  - A: People icon
  - U: Profile icon
  - T: Target icon
  - O: Document icon
  - M: Process icon
  - A: Lock icon
  - T: Speech bubble icon

**Speaker Notes:**
> AUTOMAT - seven components that ensure complete prompt specification. Audience defines who reads the output (affects technical depth). User Persona tells AI what expertise to apply. Task is the specific action. Output defines exact format. Method guides approach. Assumptions set constraints and boundaries. Tone sets voice. Together, these seven components eliminate ambiguity and enable first-shot success.

**Interactive Element:**
> **Quick Check:** "Which component do you think gets forgotten most often?" (Collect guesses)

---

## SLIDE 13: A - Audience

**Visual:**
- Different audience types illustrated
- Impact diagram

**Content:**
```
A - Audience
Define who will consume this output

Affects:
â€¢ Technical depth
â€¢ Terminology level
â€¢ Required disclaimers
â€¢ Level of detail

Examples:
â†’ IP Legal Team: Formal, cite sources, flag novelty
â†’ R&D Colleagues: Technical, assume domain knowledge
â†’ Management: High-level, business impact focus
â†’ External Partners: Balanced, avoid proprietary info
```

**Design Notes:**
- Four audience types with icons
- Arrows showing how audience affects output
- Example boxes in light gray

**Speaker Notes:**
> Audience shapes everything. Writing for lawyers vs engineers vs management requires different approaches. Lawyers need citations and formality. Engineers want technical depth. Management wants business implications. External partners need balanced detail without proprietary information. Same data, different packaging. Specifying audience upfront prevents wasted iterations reformatting for the wrong reader.

---

## SLIDE 14: U - User Persona

**Visual:**
- Example personas with expertise badges
- Before/after comparison

**Content:**
```
U - User Persona
Define AI's role and expertise

Why it matters:
Without persona â†’ "Helpful generalist" (too broad)
With persona â†’ Domain expert (targeted depth)

Examples for Materials Science:
"Act as a Senior Polymer Chemist with expertise 
in electrospinning"

"Act as a Materials Science Patent Analyst"

"Act as a Laboratory Safety Officer reviewing protocols"

"Act as a Data Scientist specializing in mechanical 
testing analysis"
```

**Design Notes:**
- Example personas in separate boxes
- "Before/after" comparison showing generic vs expert response

**Speaker Notes:**
> User Persona activates specific knowledge domains. Without it, AI defaults to "helpful generalist" - too broad for scientific work. With it, you get targeted expertise. "Act as Senior Polymer Chemist" brings polymer-specific knowledge, electrospinning context, materials science terminology. This isn't role-play; it's activating relevant training data patterns. Specificity matters - "Senior Polymer Chemist with electrospinning expertise" beats "materials scientist."

---

## SLIDE 15: T - Task

**Visual:**
- Bad example with red X
- Good example with green checkmark

**Content:**
```
T - Task
Specify the precise action required

âŒ Vague:
"Summarize this paper"
"Analyze this data"
"Help me with synthesis"

âœ… Explicit:
"Extract synthesis parameters from Section 3.2"
"Calculate tensile strength and Young's modulus from 
these 7 stress-strain curves, excluding outliers >2 SD"
"Propose 5 alternative synthesis routes that avoid 
toxic solvents"
```

**Design Notes:**
- Side-by-side comparison
- Red X and green checkmark prominent
- Highlighting specific verbs (extract, calculate, propose)

**Speaker Notes:**
> Task is where most prompts fail. "Summarize this paper" - summarize what? Key findings? Methods? All sections? Ambiguity forces AI to guess. "Extract synthesis parameters from Section 3.2" - explicit, actionable, no guessing required. Every vague task leads to iteration. Every explicit task increases first-shot success. Use specific verbs: extract, calculate, compare, propose, format, identify. Avoid: summarize, analyze, help (too vague).

**Interactive Element:**
> **Live Demo:** Show a vague prompt getting poor results, then show AUTOMAT prompt with T component getting first-shot success. (Use sandbox live)

---

## SLIDE 16: O - Output

**Visual:**
- Format examples shown visually
- Template preview

**Content:**
```
O - Output
Define exact format and structure

Common Scientific Formats:
â€¢ Markdown table with specified columns
â€¢ CSV data with headers
â€¢ JSON structure
â€¢ Bullet points with citations
â€¢ Structured report (sections defined)
â€¢ Python code (commented)

Example Specification:
"Output: Markdown table with columns:
- Polymer Type
- Synthesis Temperature (Â°C)
- Yield (%)
- Source (DOI)

Sort by: Yield (descending)
Include: 10-15 entries"
```

**Design Notes:**
- Visual examples of each format (small previews)
- Detailed specification in highlighted box

**Speaker Notes:**
> Output specification prevents reformatting iterations. Don't say "put it in a table" - specify columns, sorting, how many entries. Don't say "give me code" - specify language, commenting style, function structure. This component alone can save 3-4 iterations. AI doesn't know what format you need; you must tell it explicitly. Include units, column names, sorting criteria, number of items, everything that defines "correct format" for your use.

---

## SLIDE 17: M - Method

**Visual:**
- Process flow diagram
- Standards/methodology icons

**Content:**
```
M - Method
Specify approach or methodology

Examples for Materials Science:

"Use systematic literature review principles"

"Apply ISO 527 tensile testing standards"

"Follow IUPAC nomenclature"

"Compare results against baseline (pure PLA)"

"Calculate statistics using standard error of the mean"

"Use linear regression for modulus (strain 0.05-0.25%)"
```

**Design Notes:**
- Each example in a box with relevant icon
- Arrow showing method â†’ process â†’ output

**Speaker Notes:**
> Method guides how AI approaches the task. "Use ISO 527 standards" invokes specific testing methodology. "Follow IUPAC nomenclature" ensures correct chemical naming. "Compare against pure PLA baseline" provides reference point. Method is especially important for technical work where specific standards or conventions apply. If you have a preferred methodology, specify it. If there are multiple valid approaches, specifying method prevents AI from choosing arbitrarily.

---

## SLIDE 18: A - Assumptions (Constraints)

**Visual:**
- Constraint types with examples
- Boundary diagram

**Content:**
```
A - Assumptions & Constraints
Critical for scientific accuracy

Three Types:

1. Whitelist (What to Include)
   "Focus exclusively on non-oxide ceramics"
   "Only papers published 2020-2024"

2. Blacklist (What to Exclude)
   "Exclude review papers"
   "Ignore pre-2020 publications"

3. Uncertainty Handling
   "If data missing, mark 'Not Reported' - do not estimate"
   "If conflicting values, flag discrepancy"
   "Do not extrapolate from related materials"
```

**Design Notes:**
- Three sections clearly separated
- Examples in quote boxes
- Warning icon for uncertainty handling

**Speaker Notes:**
> Assumptions - the hallucination prevention component. Without constraints, AI fills gaps with guesses. "If data is missing, mark Not Reported - do not estimate" prevents fabrication. "Exclude review papers" prevents mixing primary and secondary sources. "Focus exclusively on electrospinning" prevents irrelevant methods. This component protects scientific accuracy. Be explicit about what to include, what to exclude, and critically, how to handle uncertainty.

---

## SLIDE 19: T - Tone

**Visual:**
- Tone spectrum with examples
- Document type icons

**Content:**
```
T - Tone
Set voice and style

Scientific Tones:

Technical/Objective â†’ For peer review, reports
Formal â†’ For legal, regulatory documents
Conversational â†’ For internal brainstorming
Cautious â†’ For safety-critical procedures

Example Specification:
"Tone: Technical and objective, suitable for 
submission to Materials Science & Engineering: A. 
Avoid speculation or promotional language."
```

**Design Notes:**
- Horizontal spectrum showing tone range
- Document icons (legal doc, journal article, report)
- Example in highlighted box

**Speaker Notes:**
> Tone sets voice and style. "Technical and objective for journal submission" produces different language than "conversational for internal brainstorming." For patents: formal, precise. For lab documentation: clear, procedural. For management reports: business-focused. Tone prevents iteration to adjust style. If you need academic tone, specify it. If you need cautious language for safety, specify it. This component fine-tunes the last 10% of quality.

---

## SLIDE 20: Complete AUTOMAT Example

**Visual:**
- Full prompt displayed
- Each component highlighted in different color

**Content:**
```
[U] Act as Materials Science Patent Analyst with expertise in 
polymer composites and prior art searches.

[A] Audience: AmaDema IP Legal Team preparing patent application 
for novel PLA/graphene synthesis method.

[T] Extract information relevant to prior art assessment from 
these 12 papers on electrospinning-based PLA/graphene synthesis.

[O] Output: Markdown table - DOI, Year, Method, Graphene %, 
Key Innovation, Overlap (High/Medium/Low/None), Legal Notes

[M] Apply patent prior art search principles. Focus on claims 
challenging patentability. Highlight electrospinning + in-situ 
reduction methods.

[A] Constraints: Only peer-reviewed articles (no reviews/patents). 
If graphene % not specified, mark "NR". Flag papers from 
competitors (Nanotech Solutions, PolyMat GmbH). No speculation.

[T] Tone: Formal, objective, suitable for legal review. 
Err on caution when assessing overlap.
```

**Design Notes:**
- Each letter labeled [A], [U], [T], etc.
- Different color highlight for each component
- Clean, readable font

**Speaker Notes:**
> Here's a complete AUTOMAT prompt. Notice how every component is explicitly stated. This is for patent analysis - Audience is legal team (affects formality). User Persona is patent analyst (activates IP search knowledge). Task is explicit (extract information for prior art). Output specifies exact table structure. Method references patent principles. Assumptions constrain scope and prevent speculation. Tone is formal for legal context. This prompt gets first-shot, production-ready results. Without AUTOMAT, you'd iterate 5+ times to get this quality.

---

## SLIDE 21: AUTOMAT in Action - Live Demo

**Visual:**
- Screen capture placeholder
- Before/after comparison

**Content:**
```
Live Demonstration

Task: Extract synthesis parameters from 
materials science paper

Without AUTOMAT:
â†’ Vague request
â†’ Generic response
â†’ Multiple iterations needed

With AUTOMAT:
â†’ Complete specification
â†’ First-shot success
â†’ Production-ready output

[LIVE SANDBOX DEMO]
```

**Design Notes:**
- Large "LIVE DEMO" banner
- Timer icon (showing time savings)
- Placeholder for screen share

**Speaker Notes:**
> Let's see this in action. I'll show you the same task twice - once casual, once AUTOMAT. Watch the difference in iterations needed and output quality. [Conduct live demo in sandbox showing vague prompt requiring multiple iterations, then AUTOMAT prompt succeeding first shot]

**Interactive Element:**
> **Live Demo:** 
> 1. Show vague prompt: "Summarize synthesis parameters from this paper"
> 2. Show poor result requiring clarification
> 3. Show complete AUTOMAT prompt
> 4. Show first-shot success with production-ready table
> 5. Emphasize time difference (30 seconds vs 5 minutes)

---

## SLIDE 22: When to Use AUTOMAT

**Visual:**
- Decision flowchart
- Use case icons

**Content:**
```
AUTOMAT is Best For:

âœ“ Functional tasks with clear outputs
  (data extraction, formatting)

âœ“ Scientific workflows requiring precision

âœ“ Tasks where IP protection is critical

âœ“ Situations requiring reproducible results

Not Ideal For:

âŒ Open-ended brainstorming

âŒ Conversational learning (use CO-STAR - Day 2)

âŒ Simple, one-sentence tasks
```

**Design Notes:**
- Two columns: Good for / Not for
- Green checkmarks and red X's
- Task icons

**Speaker Notes:**
> AUTOMAT excels at functional, structured tasks. Data extraction from papers? Perfect. Protocol formatting? Ideal. Patent analysis? Excellent. These are tasks with clear inputs and outputs where precision matters. Not ideal for brainstorming (too structured) or conversational learning (tomorrow's CO-STAR framework is better). Also overkill for "What's the melting point of PLA?" - simple questions don't need seven components. Use the right tool for the task.

---

## SLIDE 23: Section Title - Conversational Learning

**Visual:**
- Section divider with icon
- Conversation bubbles graphic

**Content:**
```
PART 3

Conversational Learning

From task executor to learning partner
```

**Design Notes:**
- Conversation bubbles in red and black
- Minimalist, clear divider

**Speaker Notes:**
> We've covered AUTOMAT for functional tasks. Now let's shift gears - using AI not just to complete tasks, but to build your expertise. This is about transforming how you interact with AI.

---

## SLIDE 24: The Mindset Shift

**Visual:**
- Two-path diagram showing different approaches
- Transformation arrow

**Content:**
```
Transactional âŒ â†’ Learning Partnership âœ…

Transactional:
"Summarize this paper" â†’ [Copy summary] â†’ Done
Result: Summary obtained, no learning

Learning Partnership:
"Summarize this paper" â†’ [Read summary]
"Why did you structure it this way?"
"What criteria determined 'key findings'?"
"How would you adapt this for ceramic papers?"
Result: Summary + Methodology understanding

Same time, 5Ã— learning
```

**Design Notes:**
- Left path (transactional) in gray, ending at dead end
- Right path (learning) in red, branching into multiple insights
- Comparison showing same time investment, different outcomes

**Speaker Notes:**
> Most people use AI transactionally - get answer, copy it, move on. No learning occurs. Learning partnership approach adds "why" questions. You invest same time but gain transferable understanding. The summary is a side effect; the methodology is the value. This compounds - today's learning applies to tomorrow's tasks. Transactional users need AI every time. Learning users become increasingly independent.

---

## SLIDE 25: The Power of "Why" Questions

**Visual:**
- Question hierarchy pyramid
- Examples at each level

**Content:**
```
Basic Learning Question Types:

1. Process Questions
   "How did you reach that conclusion?"
   "Walk me through your thinking"

2. Justification Questions
   "Why this approach over alternatives?"
   "What assumptions underlie your recommendation?"

3. Alternative Questions
   "What other approaches could work?"
   "How would this change in different contexts?"

Start simple, build depth progressively
```

**Design Notes:**
- Three tiers, progressively deeper
- Example questions in quote boxes
- Arrow showing progression from surface to depth

**Speaker Notes:**
> Three basic question types to start. Process questions reveal methodology. Justification questions expose assumptions. Alternative questions explore trade-offs. You don't need fancy techniques - just add one follow-up "why" to your normal AI interaction and you've started learning. "You recommended DMF - why?" reveals solvent selection criteria you can apply to future decisions. Small investment, large returns.

---

## SLIDE 26: Conversational Learning Example

**Visual:**
- Dialogue visualization
- Knowledge gained highlighted

**Content:**
```
Practical Example: Polymer Synthesis

Transactional (30 seconds):
User: "Best method to synthesize PLA?"
AI: "Ring-opening polymerization with tin catalysts"
User: "Thanks"
Knowledge gained: Zero

Learning Partnership (5 minutes):
User: "Best method to synthesize PLA?"
AI: "Ring-opening polymerization (ROP) of lactide..."
User: "Why ROP over direct polycondensation?"
AI: "ROP achieves higher Mw (100-300 kDa vs 10-30 kDa)..."
User: "What trade-offs exist? When would polycondensation be better?"
AI: "Polycondensation simpler and cheaper for lower Mw applications..."
User: "You mentioned tin catalysts. What alternatives and why aren't they standard?"
AI: "Al, Zn, Mg catalysts avoid tin toxicity concerns..."

Knowledge gained: Decision framework, trade-off understanding, transferable methodology
```

**Design Notes:**
- Two columns comparing approaches
- Highlighted "Knowledge gained" boxes
- Time noted for each

**Speaker Notes:**
> Same starting question, radically different outcomes. Transactional: 30 seconds, got a fact, zero learning. Learning partnership: 5 minutes, got the fact PLUS decision framework, trade-off understanding, catalyst selection criteria. That 5 minutes of investment pays dividends on every future synthesis decision. You're not just collecting answers; you're building expertise.

**Interactive Element:**
> **Think-Pair-Share:** "Think of one technical question you've asked AI recently. What follow-up 'why' question could you have asked?" (2 minutes discussion)

---

## SLIDE 27: Section Title - Responsible AI & IP Protection

**Visual:**
- Security/lock icon
- Red warning banner

**Content:**
```
PART 4

Responsible AI & 
IP Protection

Protecting AmaDema while leveraging AI
```

**Design Notes:**
- Bold, serious visual design
- Lock icon in red
- Professional, formal tone

**Speaker Notes:**
> Critical section. AI tools are powerful, but they come with risks. You can leverage AI's capabilities while protecting AmaDema's intellectual property. This isn't optional compliance - it's professional responsibility.

---

## SLIDE 28: The Red List

**Visual:**
- Warning triangle
- List with red prohibition icons

**Content:**
```
ðŸš« THE RED LIST ðŸš«
NEVER Share with Public AI Tools

1. Unpublished Research Data
   â€¢ Novel molecular structures
   â€¢ Exact synthesis parameters (proprietary)
   â€¢ Experimental results (ongoing R&D)

2. Commercial Sensitive Information
   â€¢ Exact formulations
   â€¢ Yield data
   â€¢ Customer identities
   â€¢ Pricing strategies

3. Personal & Confidential Data
   â€¢ Employee information
   â€¢ Internal communications
   â€¢ Financial data

4. Security-Sensitive Information
   â€¢ Access credentials
   â€¢ System configurations
```

**Design Notes:**
- Red warning banner at top
- Prohibition icons (red circle with slash) for each category
- Bold, attention-grabbing format

**Speaker Notes:**
> The Red List - non-negotiable. These data types NEVER go to ChatGPT, Claude, Gemini, or any public AI tool. Unpublished research loses patent priority if disclosed. Customer names violate NDAs. Exact formulations are trade secrets. Once shared with external AI, data may be stored, logged, or even used for training. For Red List items, use our local sandbox only - data never leaves the laptop. If you're unsure whether something is Red List, assume yes and ask IT.

---

## SLIDE 29: Hallucinations - What They Are

**Visual:**
- AI model making things up (illustration)
- Example fabrication

**Content:**
```
Understanding Hallucinations

What: AI generates plausible-sounding but 
factually incorrect information

Why They Occur:
LLMs are pattern-matching prediction engines, 
NOT knowledge databases

When you ask: "What is the tensile strength of 
Tiâ‚†Alâ‚„V at 400Â°C with 2% oxygen?"

AI doesn't:
âŒ Look it up in database
âŒ Calculate from first principles
âŒ Admit uncertainty

AI does:
âœ… Predict most probable tokens
âœ… Synthesize plausible-sounding answer
âœ… Present with confidence

â†’ Confidence â‰  Accuracy
```

**Design Notes:**
- Visual showing AI "guessing" with question marks
- Red warning icon for confidence issue
- Clear "doesn't/does" comparison

**Speaker Notes:**
> Hallucinations are AI's biggest risk for scientific work. AI doesn't "know" things - it predicts patterns. Asked for specific Ti alloy data at specific conditions, it doesn't look it up or calculate it. It generates plausible-sounding text based on training patterns. And critically, it presents guesses with confidence. Your job: verify everything. Especially citations, quantitative values, and specific claims. Never trust, always verify.

**Interactive Element:**
> **Poll:** "Has anyone encountered an AI hallucination in their work?" (Show of hands, brief discussion of examples)

---

## SLIDE 30: Preventing Hallucinations

**Visual:**
- Shield icon with strategies
- Before/after constraint examples

**Content:**
```
Hallucination Prevention Through Constraints

Without Constraints âŒ:
"What is the melting point of this polymer?"
â†’ AI guesses if not in paper

With Constraints âœ…:
"If melting point is reported in paper, extract it 
with page citation. If not reported, state 'Not Reported' 
- do not estimate."
â†’ AI admits ignorance instead of guessing

Key Strategies:
â€¢ Explicit uncertainty handling
â€¢ Require citations for claims
â€¢ Demand "Not Reported" over estimation
â€¢ Flag missing data, don't fill gaps
```

**Design Notes:**
- Side-by-side comparison
- Highlighted constraint language
- Red X and green checkmark

**Speaker Notes:**
> You prevent hallucinations through explicit constraints - the A component of AUTOMAT. "If data is missing, mark Not Reported - do not estimate" turns off guessing. "Cite page numbers for all claims" forces grounding in actual text. "If conflicting values, flag discrepancy" prevents AI from choosing arbitrarily. These constraints are your scientific rigor applied to AI. Don't let AI speculate; make it admit uncertainty.

---

## SLIDE 31: Verification Protocols

**Visual:**
- Checklist with verification steps
- Tool logos (CrossRef, Google Scholar)

**Content:**
```
Never Trust, Always Verify

Citation Verification:
â˜ DOI resolves to real paper
â˜ Authors match
â˜ Journal and year correct
â˜ Abstract aligns with claimed content
â˜ Check actual data in paper

Quantitative Verification:
â˜ Value within physically reasonable range
â˜ Units are consistent
â˜ Compare to known benchmarks
â˜ Check against authoritative sources

Tools:
â†’ CrossRef DOI Search
â†’ Google Scholar
â†’ PubMed
â†’ Materials handbooks
â†’ Your domain expertise
```

**Design Notes:**
- Two checklists side by side
- Tool logos at bottom
- Professional, systematic layout

**Speaker Notes:**
> Verification is non-negotiable. For citations: verify DOI works, check authors match, read the abstract to confirm AI's claim aligns. For numbers: sanity check (PLA melting at 300Â°C? Impossible). Check units, compare to benchmarks. Use CrossRef for DOIs, Google Scholar for quick checks, your handbooks for reference values. Most importantly, apply your domain expertise. If it sounds wrong, it probably is. Trust but verify becomes: don't trust, always verify.

---

## SLIDE 32: Data Sanitization Strategies

**Visual:**
- Data transformation examples
- Before/after sanitization

**Content:**
```
Using AI Safely with Sensitive Data

Strategy 1: Anonymization
Replace specifics with placeholders

âŒ Specific: "25kV, 15cm, 12% PLA (Mw 100kDa) 
in DMF:DCM (3:1), 5% graphene, 80Â°C, Yield: 87%"

âœ… Sanitized: "[VOLTAGE], [DISTANCE], 
[CONCENTRATION]% polymer (Mw [VALUE]) 
in [SOLVENT], [X]% nanofiller, [TEMPERATURE], 
Yield: [HIGH/MEDIUM/LOW]"

Strategy 2: Aggregation
Share trends, not specific values

âŒ "Batch #343: 45.2 MPa at 23Â°C"
âœ… "Strength decreases ~15% between 20-60Â°C"

Strategy 3: Hypothetical Framing
Ask about principles, not your specific case
```

**Design Notes:**
- Three strategies clearly separated
- Red X / Green checkmark comparisons
- Highlighted placeholders in brackets

**Speaker Notes:**
> You CAN use external AI for sensitive tasks if you sanitize first. Anonymization: replace exact values with [PLACEHOLDERS]. Now you can ask "format this generic protocol" without revealing your proprietary conditions. Aggregation: share trends not data points. Hypothetical framing: "For polymer nanocomposites generally, what affects dispersion?" instead of "For our PLA/graphene at these exact conditions..." Sanitization lets you leverage AI while protecting IP.

---

## SLIDE 33: The Local Sandbox Advantage

**Visual:**
- Laptop with local AI icon
- Security checkmarks

**Content:**
```
For Red List Items: Use Local Sandbox

How it Works:
â€¢ Ollama + Open WebUI running on instructor laptop
â€¢ Llama 3.2 3B model (fully local)
â€¢ Connected via local WiFi (no internet)

Benefits:
âœ… Data never leaves the laptop
âœ… No external servers
âœ… No logging by third parties
âœ… No internet required
âœ… Full privacy for sensitive work

Access: http://[instructor-IP]:3000

Best For:
â†’ Proprietary synthesis data
â†’ Patent applications
â†’ Customer information
â†’ Any Red List item
```

**Design Notes:**
- Diagram showing local network
- Security shield icons
- Access URL prominently displayed

**Speaker Notes:**
> For Red List items, we have the local sandbox. Ollama and Open WebUI running on my laptop, Llama 3.2 model loaded. You connect via local WiFi - no internet involved. Your prompts and data never leave this room. No OpenAI servers, no Anthropic, no Google - just local processing. It's private, secure, and perfect for sensitive work. Access via the URL I'll provide. Quality is excellent for most tasks; it's a capable 3 billion parameter model.

**Interactive Element:**
> **Live Demo:** Show sandbox access, demonstrate a query, emphasize privacy (nothing leaves the room)

---

## SLIDE 34: Section Title - Exercises

**Visual:**
- Hands-on practice icon
- Workshop tools graphic

**Content:**
```
TIME TO PRACTICE

Hands-On Exercises

Applying what you've learned
```

**Design Notes:**
- Bold, energizing design
- Tool/workshop icon in red
- Large text inviting participation

**Speaker Notes:**
> Theory done. Time to practice. You'll work through exercises that apply AUTOMAT, conversational learning, and responsible AI principles. These are real materials science scenarios. You'll use the sandbox for some, work individually for others, and we'll debrief as a group.

---

## SLIDE 35: Exercise 1 - AUTOMAT for Literature Review

**Visual:**
- Exercise icon with "1"
- Task description

**Content:**
```
Exercise 1: Build Your First AUTOMAT Prompt

Scenario:
You have 8 papers on electrospun PLA/graphene 
nanocomposites. Need to extract synthesis parameters 
for competitive analysis (quarterly R&D review).

Your Task:
Construct complete AUTOMAT prompt that:
â€¢ Extracts key synthesis parameters
â€¢ Outputs structured format for comparison
â€¢ Includes constraints for consistency
â€¢ Handles missing data appropriately

Time: 15 minutes
Location: Use sandbox at http://[IP]:3000
```

**Design Notes:**
- Clear scenario box
- Task checklist
- Time and location prominent
- Exercise number in red circle

**Speaker Notes:**
> First exercise: build your own AUTOMAT prompt. Scenario is literature review for competitive analysis - realistic R&D task. You need to extract synthesis parameters, specify output format, add constraints. Refer to your cheat sheet (website has it). Work individually first, then we'll review examples together. If you get stuck, raise your hand. Focus on being explicit in each component - that's the skill we're building.

**Interactive Element:**
> Walk around, provide individual guidance, note common mistakes for group debrief

---

## SLIDE 36: Exercise 1 - Debrief

**Visual:**
- Example solution displayed
- Common mistakes highlighted

**Content:**
```
Exercise 1: What We Learned

Common Mistakes:
âŒ Vague Task: "Extract info from papers"
âœ… Specific: "Extract synthesis temp, voltage, 
   graphene %, yield from Methods sections"

âŒ Undefined Output: "Make it a table"
âœ… Specific: "Markdown table with columns: 
   [DOI, Method, Temp (Â°C), Voltage (kV)...]"

âŒ No Constraints: AI guesses when data missing
âœ… Explicit: "If parameter not reported, mark 'NR'"

Key Takeaway:
Specificity eliminates iterations
```

**Design Notes:**
- Red X / Green checkmark comparisons
- Highlighted example solution
- Key takeaway in box

**Speaker Notes:**
> Let's debrief. Common mistakes: vague tasks, undefined outputs, missing constraints. Look at the difference between "extract info" and "extract synthesis temperature, voltage, graphene percentage from Methods sections only." The specific version eliminates guessing. Same with output - "make it a table" leaves format ambiguous. "Markdown table with these exact columns" is actionable. Anyone want to share their prompt? [Invite volunteers to share, provide constructive feedback]

**Interactive Element:**
> **Volunteer Share:** "Who wants to share their prompt with the group?" (2-3 volunteers, provide feedback, highlight good practices)

---

## SLIDE 37: Exercise 2 - The Hallucination Hunt

**Visual:**
- Detective/magnifying glass icon
- Challenge banner

**Content:**
```
Exercise 2: Find the Errors

Challenge:
AI-generated synthesis report contains 
5 deliberate errors:
â€¢ 1 fabricated citation
â€¢ 1 physically impossible value
â€¢ 1 misattributed discovery
â€¢ 1 inconsistent claim
â€¢ 1 non-existent characterization technique

Your Task:
1. Read report (on website)
2. Identify all 5 errors
3. Explain why each is wrong
4. Suggest verification method

Time: 20 minutes
Prize: "Critical Thinker" badge for fastest team
```

**Design Notes:**
- Challenge/game aesthetic
- Error types listed clearly
- Prize mentioned for motivation
- Timer graphic

**Speaker Notes:**
> Hallucination hunt - the fun exercise. You're detectives finding errors in an AI-generated report. Five deliberate mistakes hidden in realistic-looking scientific text. This trains your verification skills. Work in pairs if you want. Report is on the website under Day 1 exercises. Some errors are obvious (fabricated DOI), others subtle (physically possible but wrong). Use your domain knowledge, check citations, apply sanity checks. First team to find all 5 wins bragging rights.

**Interactive Element:**
> Monitor time, provide hints if groups are stuck, create competitive atmosphere

---

## SLIDE 38: Exercise 2 - Debrief & Answers

**Visual:**
- Reveal answers with explanations
- Verification methods

**Content:**
```
Hallucination Hunt: Answers

Error 1: Fabricated Citation
Zhang et al. (2023), DOI: 10.1002/pen.2023.08.445
â†’ DOI format wrong, doesn't resolve
â†’ Verify: CrossRef search

Error 2: Physically Impossible
Simultaneous modulus AND ductility increase
â†’ Typically trade-off (stiffness â†” flexibility)
â†’ Verify: Materials science principles

Error 3: Non-existent Technique
"Quantum-resolution XPS"
â†’ No such characterization method exists
â†’ Verify: Domain knowledge

[Continue for all 5...]
```

**Design Notes:**
- Each error revealed separately
- Verification method for each
- Red highlight on errors

**Speaker Notes:**
> Let's reveal the errors. Error 1: fabricated citation. DOI format is wrong (2023.08.445 isn't valid format). Try it in CrossRef - doesn't exist. This is a common hallucination - realistic-looking but fake citation. Error 2: simultaneous modulus and ductility increase. Your materials science knowledge should flag this - typically a trade-off. Possible but unusual, requires verification. Error 3: "quantum-resolution XPS" - no such technique. AI invented it combining "quantum" (sounds scientific) with XPS (real technique). Your domain knowledge catches this immediately. [Continue through all 5]

**Interactive Element:**
> **Group Discussion:** "Which error was hardest to spot? Which was easiest?" (Discuss verification strategies)

---

## SLIDE 39: Exercise 3 - Red List Assessment

**Visual:**
- Traffic light system (red/yellow/green)
- Scenario cards

**Content:**
```
Exercise 3: IP Protection Practice

For each scenario, determine:
âœ… Green: Safe for external AI
âš ï¸ Yellow: Safe after sanitization
ðŸš« Red: Local sandbox only

Scenarios:
A. Format lab notebook (exact synthesis conditions, 
   patent-pending process)
B. Summarize 6 published papers (DOIs provided)
C. Analyze SEM image (from proprietary experiment)
D. Generate Python code (generic, no data attached)
E. Draft customer email (names company, revenue %)

Time: 10 minutes
```

**Design Notes:**
- Traffic light icons prominent
- Scenarios in separate cards
- Color-coded for quick reference

**Speaker Notes:**
> Red List practice. Five scenarios, three levels of risk. Green: safe for ChatGPT/Claude. Yellow: sanitize first, then safe. Red: local sandbox only. Read each carefully - details matter. "Published papers" = green (public data). "Exact synthesis conditions for patent-pending process" = red (unpublished IP). "Customer names and revenue %" = red (confidential). Think through each scenario, apply Red List principles. This becomes second nature with practice.

---

## SLIDE 40: Exercise 3 - Debrief & Answers

**Visual:**
- Answers revealed with explanations
- Decision framework

**Content:**
```
Red List Assessment: Answers

Scenario A: ðŸš« RED
Patent-pending = unpublished IP
â†’ Local sandbox only

Scenario B: âœ… GREEN
Published papers = public data
â†’ Any tool fine

Scenario C: ðŸš« RED
Proprietary experiment = Red List
â†’ Local sandbox OR sanitize image (remove metadata)

Scenario D: âœ… GREEN
Generic code request, no data = public info
â†’ Any tool fine

Scenario E: ðŸš« RED
Customer name + revenue = confidential
â†’ Local sandbox OR sanitize (remove names/numbers)
```

**Design Notes:**
- Each answer with color-coded icon
- Reasoning provided
- Sanitization options noted

**Speaker Notes:**
> Answers: A is red - patent-pending means unpublished, never external AI. B is green - published papers are public, no issue. C is red unless sanitized - proprietary image is Red List, but you could strip metadata and sanitize. D is green - generic code request with no attached data is fine. E is red unless sanitized - customer names and revenue percentages are confidential. Remove specifics, ask hypothetically instead. The pattern: published/public = green, unpublished/confidential = red, unless you can sanitize to remove sensitive details.

---

## SLIDE 41: Key Takeaways - Day 1

**Visual:**
- Summary checklist
- Key points highlighted

**Content:**
```
What You've Learned Today

âœ“ Why prompt engineering matters
  (50-75% efficiency gains possible)

âœ“ AUTOMAT framework
  (Seven components for functional tasks)

âœ“ Conversational learning
  (Build expertise, not just collect answers)

âœ“ IP protection
  (Red List + verification protocols)

âœ“ Practical skills
  (Exercises completed in sandbox)

Tomorrow: Context mastery & CO-STAR framework
```

**Design Notes:**
- Large checkmarks in red
- Clean, celebratory layout
- Preview of Day 2 at bottom

**Speaker Notes:**
> Let's recap. You now understand why prompt engineering isn't optional - it's professional skill delivering 50-75% efficiency gains. You've learned AUTOMAT - seven components ensuring first-shot success on functional tasks. You've practiced conversational learning - using "why" questions to build expertise. You know the Red List and verification protocols - protecting AmaDema's IP while leveraging AI. And you've practiced in the sandbox with real scenarios. Tomorrow, we go deeper into context and learn CO-STAR framework for strategic communication. Excellent work today!

---

## SLIDE 42: Action Items Before Day 2

**Visual:**
- Checklist with timeline
- Resource links

**Content:**
```
Before Day 2 (Optional but Helpful)

â˜ Review Day 1 content on website
  (Especially AUTOMAT examples)

â˜ Try one AUTOMAT prompt in your real work
  (Template something you do regularly)

â˜ Note questions or challenges
  (We'll address at Day 2 start)

â˜ Optional pre-reading:
  â†’ Context Matters
  â†’ What are LLMs?
  â†’ LLM Tools Comparison

Website: [URL]
Password: AmaDema2026
```

**Design Notes:**
- Checkbox list
- Website credentials prominent
- "Optional" clearly stated

**Speaker Notes:**
> Optional homework before Day 2. Review today's content on the website - especially AUTOMAT examples. If you feel ambitious, try one AUTOMAT prompt in your real work before tomorrow. Template something you do regularly. Note any questions or challenges - we'll start Day 2 addressing those. There's also optional pre-reading on context and LLM basics. Website credentials on screen. But if you do nothing, that's fine - Day 2 will recap what's needed.

---

## SLIDE 43: Thank You & Questions

**Visual:**
- AmaDema logo
- Contact information
- Q&A icon

**Content:**
```
Thank You!

Questions?

Contact:
â€¢ Website: [URL]
â€¢ Instructor: [Email]
â€¢ Sandbox access: Available during all sessions

See you for Day 2:
Context & CO-STAR Framework

[AmaDema Logo]
```

**Design Notes:**
- Clean, professional closing slide
- Logo centered
- Contact info visible
- Q&A invitation

**Speaker Notes:**
> Excellent work today! Before we wrap, let's take questions. Anything unclear? Anything you want to practice more? Sandbox remains available - you can keep practicing after we end. Day 2 tomorrow, we'll dive into context mastery and CO-STAR framework. Thank you for your engagement and great questions today!

**Interactive Element:**
> **Open Q&A:** Take all remaining questions, provide clarification, invite feedback on today's session

---

## END OF DAY 1 SLIDES

---

## APPENDIX: Facilitation Notes

### Timing Guidance (Flexible)
- Welcome & Agenda: 10 min
- Intro to Prompt Engineering: 20 min
- AUTOMAT Framework: 35 min
- Conversational Learning: 15 min
- Responsible AI: 20 min
- Exercises: 30-40 min
- Debrief & Wrap: 10 min
**Total: ~90 minutes (flexible)**

### Interactive Elements Summary
- 6 Polls/Show of hands
- 4 Think-Pair-Share activities
- 3 Live demonstrations
- 3 Group discussions
- 1 Volunteer sharing session
- Exercise facilitation throughout

### Materials Needed
- Laptop with sandbox running
- Projector/screen for slides
- Whiteboard for ad-hoc explanations
- Participants need: laptops, website access, sandbox URL
- Handouts: Cheat sheet (optional, on website)

### Troubleshooting Tips
- If sandbox slow: reduce participants per query
- If exercise too hard: provide AUTOMAT template
- If exercise too easy: add complexity challenge
- If running over time: skip Exercise 3, assign as homework
- If ahead of time: add more live demo examples

### Adaptation Notes
- Slides are detailed but flexible - adjust to group pace
- Interactive elements are suggestions - read the room
- Technical level assumes materials science PhDs - adjust examples if different audience
- Emphasize AmaDema-specific examples when possible

---

**END OF INSTRUCTOR SLIDES DOCUMENT**
